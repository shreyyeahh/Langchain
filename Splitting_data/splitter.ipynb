{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476ee25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "## reading a pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "docs = loader.load() # loading the content of the pdf file\n",
    "print(docs[0].page_content) # Displaying the content of the first document loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b829d",
   "metadata": {},
   "source": [
    "How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4dc80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Splitting the text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500 , \n",
    "    chunk_overlap = 50)\n",
    "final_documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70962633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "page_content='University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0]) # Displaying the content of the first chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb288c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[1]) # Displaying the metadata of the second chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e8b1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Loader\n",
    "# This script is used to load text files and process them for further use in the Langchain framework.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\n",
    "with open('speech.txt','r') as file:\n",
    "    speech = file.read()   \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100, chunk_overlap = 20\n",
    ")  \n",
    "text= text_splitter.create_documents([speech])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b56a05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 400\n",
      "Created a chunk of size 670, which is longer than the specified 400\n",
      "Created a chunk of size 984, which is longer than the specified 400\n",
      "Created a chunk of size 791, which is longer than the specified 400\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "speech = \"\"\n",
    "with open('speech.txt','r') as file:\n",
    "    speech = file.read()   \n",
    "\n",
    "text_splitter =   CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 400 , \n",
    "    chunk_overlap = 40\n",
    "    )  \n",
    "final_documents = text_splitter.create_documents([speech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a00ef6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End container NOTE: Script required for drop-down button to work (mirrors).  \n",
      "End header wrapper End content End footer  \n",
      "End header  \n",
      "End navigation End search  \n",
      "Stanford Encyclopedia of Philosophy  \n",
      "Menu  \n",
      "Browse  \n",
      "Table of Contents  \n",
      "What's New  \n",
      "Random Entry  \n",
      "Chronological  \n",
      "Archives  \n",
      "About  \n",
      "Editorial Information  \n",
      "About the SEP  \n",
      "Editorial Board  \n",
      "How to Cite the SEP  \n",
      "Special Characters  \n",
      "Advanced Tools  \n",
      "Contact  \n",
      "Support SEP  \n",
      "Support the SEP  \n",
      "PDFs for SEP Friends  \n",
      "Make a Donation  \n",
      "SEPIA for Libraries  \n",
      "Begin article sidebar End article sidebar NOTE: Article content must have two wrapper divs: id=\"article\" and id=\"article-content\" End article NOTE: article banner is outside of the id=\"article\" div. End article-banner  \n",
      "Entry Navigation  \n",
      "Entry Contents  \n",
      "Bibliography  \n",
      "Academic Tools  \n",
      "Friends PDF Preview  \n",
      "Author and Citation Info  \n",
      "Back to Top  \n",
      "End article-content  \n",
      "BEGIN ARTICLE HTML #aueditable DO NOT MODIFY THIS LINE AND BELOW END ARTICLE HTML  \n",
      "DO NOT MODIFY THIS LINE AND ABOVE\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Fetch HTML content using requests\n",
    "import requests\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/plato/\"\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\")]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "result = html_splitter.split_text_from_url(url)\n",
    "print(result[0].page_content)  # Displaying the split content based on headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f785a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "url = \"https://plato.stanford.edu/entries/plato/\"\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\")]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "result = html_splitter.split_text(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "211c5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Example JSON data\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openai.json\").json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1ea615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail': 'Not Found'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
